#kafka.move.to.earliest.offset=true
#
# Destination paths
#
etl.destination.path=hdfs://HOST/user/camus/data
etl.execution.base.path=hdfs://HOST/user/camus/base
etl.execution.history.path=hdfs://HOST/user/camus/history
#
# Encoders
#
#camus.message.encoder.class=com.linkedin.camus.etl.kafka.coders.DummyKafkaMessageEncoder
#
# Decoders: JsonStringMessageDecoder, LatestSchemaKafkaAvroMessageDecoder
#
#camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.LatestSchemaKafkaAvroMessageDecoder
camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.StringMessageDecoder

#
# Avro Schemas
#
#kafka.message.coder.schema.registry.class=com.linkedin.camus.example.DummySchemaRegistry
#kafka.message.coder.schema.registry.class=com.linkedin.camus.example.schemaregistry.DummySchemaRegistry

etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.StringRecordWriterProvider
etl.output.record.delimiter=\n
# Properties used for JsonStringMessageDecoder
#camus.message.timestamp.format=yyyy-MM-dd'T'HH:mm:ss
#camus.message.timestamp.field=dt

# Used by the committer to arrange .avro files into a partitioned scheme. 
# This will be the default partitioner for all topic that do not have a 
# partitioner specified
#etl.partitioner.class=com.linkedin.camus.etl.kafka.partitioner.DefaultPartitioner
etl.partitioner.class=com.linkedin.camus.etl.kafka.partitioner.TimeBasedPartitioner
etl.destination.path.topic.sub.dirformat=YYYY/MM

# Partitioners can also be set on a per-topic basis
#etl.partitioner.class.<topic-name>=com.your.custom.CustomPartitioner

# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=64

# KAFKA PROPERTIES
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=-1

# events with a timestamp older than this will be discarded. 
# kafka.max.historical.days=-1

# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1

# if whitelist has values, only whitelisted topic are pulled.  nothing on the blacklist is pulled
kafka.blacklist.topics=
kafka.whitelist.topics=
log4j.configuration=true

# Name of the client as seen by kafka
kafka.client.name=camusmensual

# Fetch Request Parameters
kafka.fetch.buffer.size=1048576
kafka.fetch.request.correlationid=
kafka.fetch.request.max.wait=
kafka.fetch.request.min.bytes=

# Connection parameters.
kafka.brokers=kafka01.ukdl.local:9092,kafka02.ukdl.local:9092,kafka03.ukdl.local:9092,kafka04.ukdl.local:9092
#kafka.timeout.value=30000

# Stops the mapper from getting inundated with Decoder exceptions for the same topic
# Default value is set to 10
# max.decoder.exceptions.to.print=5

# Controls the submitting of counts to Kafka
# Default value set to true
post.tracking.counts.to.kafka=true
#monitoring.event.class=class.that.generates.record.to.submit.counts.to.kafka

# everything below this point can be ignored for the time being, 
# will provide more documentation down the road
################################################################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10
etl.hourly=hourly
etl.daily=daily
etl.ignore.schema.errors=true

# Configure output compression. Defaults to deflate
mapreduce.output.fileoutputformat.compress=true

# For StringRecordWriterProvider use:
etl.output.codec=org.apache.hadoop.io.compress.DeflateCodec
#etl.output.codec=org.apache.hadoop.io.compress.SnappyCodec
#etl.output.codec=org.apache.hadoop.io.compress.BZip2Codec
#etl.output.codec=org.apache.hadoop.io.compress.SnappyCodec
#etl.output.codec=org.apache.hadoop.io.compress.Lz4Codec
#etl.output.codec=com.hadoop.compression.lzo.LzopCodec
#Some codecs suppor deflate level
#etl.deflate.level=6
#....

etl.default.timezone=Europe/London
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8


#mapred.output.compress=true
mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

#zookeeper.session.timeout=
#zookeeper.connection.timeout=
